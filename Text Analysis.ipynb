{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Text Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"A word is characterized by the company it keeps\"\n",
    "\n",
    "In this guide, we examine the question of how we can objectively assign a useful numerical value to words. While we can easily find a simple numerical conversion of a word by changing each letter to some integer value, this doesn't really provide us any new information and in fact just becomes another way of writing the word. \n",
    "How about when humans encounter a new word? What's a natural question you may ask when presented with a word you've never heard before? Well, if you were reading a book and saw a new word, if you didn't immediately have a dictionary on hand you would try to use context clues to figure out the meaning. This is essentially the idea behind word embedding, take a very large group of words (say the words in the english dictionary) and group them by how often they occur together in some large set of documents (a 'corpus').\n",
    "\n",
    "Consider the following example\n",
    "\n",
    "Text = \"ask not what your country can do for you, ask what you can do for your country\"\n",
    "\n",
    "Context('not') = [ask,what]\n",
    "\n",
    "Context('your') = [for,country]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "Given a large corpus of documents, we can create a vocabulary for it. Essentially, just an ordered list of the unique words it contains. For a single sentence (or even a single document) this is not very useful, the document may contain words specific to a certain topic or event. However, if we gather a large set of representative documents, we can get a vocabulary that gives us a good idea of the types of words in common usage. While we can simply take all the words in a dictionary, this may be far too large to be possible, or far too large to be useful.\n",
    "\n",
    "Let's look at a sample.\n",
    "\n",
    "After getting hold of a large corpus of data (such as the Brown corpus), we will end up with a vocabulary, or list of unique words that show up, that would look as follows\n",
    "\n",
    "['aardvark', 'able', 'already', 'are' .. 'humans','hunters' ... 'zebra','zenith'..]\n",
    "\n",
    "Now, given a new piece of text, we can represent it as a vector by placing 1's at the indexes in the vocabulary where each word of the text shows up.\n",
    "\n",
    "For example,\n",
    "\n",
    "text = \"aardvarks are avid hunters\"\n",
    "\n",
    "text = [1,0,0,1 .. 0,1, .. 0,0..]\n",
    "\n",
    "Now for each piece of text or document, we have a one-hot encoded vector for it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vector Space Model and Document Similarity\n",
    "\n",
    "An important problem in text mining is how to compute the 'similarity' between two documents. There are many important applications of this, the principal among them being search engines. If we treat the search phrase as a 'document' and the set of all web pages on the internet as other documents, we can quickly see not only how important this problem is, but also how difficult it seems. \n",
    "\n",
    "Let's start with a simple idea and build up to using document vectors in a vector space model for comparing documents\n",
    "\n",
    "If we had only two words in our vocabulary, we could represent any document as a two digit combination of 0 and 1. Sidelining the lack of usefulness of a two word vocabulary, we could graph the two digit representation of any document on a 2D plane\n",
    "\n",
    "\n",
    "We can expand this into 3 dimensions if we use 3 words\n",
    "\n",
    "\n",
    "Studying the images above we might be able to imagine that document vectors close together in the plane could represent similar documents. This is essentially the idea behind the Vector Space Model, represent documents as vectors in a plane and then use the distance between these vectors as a measure of document similarity.\n",
    "\n",
    "One way to define the distance between two vectors is to use the Dot Product. This is essentially just multiplying corresponding elements and taking the sum. For example, consider a 3 word vocabulary and consider 3 documents A, B and C. First, let's calculate the document vectors below, given our 3 word vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a fox hunt going on\n",
      "[1, 0, 1]\n",
      "I personally wouldn't hunt a fox, but their fur is quite valuable\n",
      "[1, 1, 1]\n",
      "There are many species of elephent that are becoming endangered\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "Vocabulary = ['fox','fur','hunt']\n",
    "\n",
    "def document_vector(document,vocabulary):\n",
    "    doc_vector = [0]*len(vocabulary)\n",
    "    for term in list(range(0,len(vocabulary))):\n",
    "        if vocabulary[term] in document:\n",
    "            doc_vector[term] = 1\n",
    "        else:\n",
    "            doc_vector[term] = 0\n",
    "    return(doc_vector)\n",
    "\n",
    "A = \"There was a fox hunt going on\"\n",
    "print(A)\n",
    "print(document_vector(A,vocabulary=Vocabulary))\n",
    "\n",
    "B = \"I personally wouldn't hunt a fox, but their fur is quite valuable\"\n",
    "print(B)\n",
    "print(document_vector(B,vocabulary=Vocabulary))\n",
    "\n",
    "C = \"There are many species of elephent that are becoming endangered\"\n",
    "print(C)\n",
    "print(document_vector(C,vocabulary=Vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now let's take the dot product of the document vectors for A and B:\n",
    "\n",
    "A.B = (1*1) + (0*1) + (1*1) = 2\n",
    "\n",
    "B.C = (1*0) + (1*0) + (1*0) = 0\n",
    "\n",
    "\n",
    "The acute reader might notice that when our document vectors are just one-hot encoded, the dot product just represents how many words matched between the two documents. While there are shortcomings of just using word matching to represent document similarity, it is a fairly intuistic method and we've come up with a mathematical representation of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency, Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "We came up with an interesting model for comparing the similarity of documents above, but there are certain shortcomings of this model that may not be immediately obvious. Consider the following\n",
    "\n",
    "Vocabulary = [\"What\", \"are\", \"some\", \"good\", \"radio\", \"technologies\"]\n",
    "\n",
    "Q = \"What are some good radio technologies?\"\n",
    "\n",
    "D1 = \"What are some good restaurants in the San Francisco area? Here's a list of our favorites\"\n",
    "D2 = \"Many good radio technologies have come out in the last decade\"\n",
    "\n",
    "\n",
    "It is obvious to a person that website 2 would be the much better option to return to this user. But using the Vector Space model above we get this:\n",
    "\n",
    "Q = [1,1,1,1,1,1]\n",
    "D1 = [1,1,1,0,0,0]\n",
    "D2 = [0,0,0,1,1,1]\n",
    "\n",
    "Q.D1 = (1*1) + (1*1) + (1*1) + (1*0) + (1*0) + (1*0) = 3\n",
    "Q.D2 = (1*0) + (1*0) + (1*0) + (1*1) + (1*1) + (1*1) = 3\n",
    "\n",
    "The Vector Space Model disagrees with our assesement and says both webpages are equally relevant. This is not good news, but let's try to diagnose why this might have happened and how we might fix it. After some thought, you may observe that all words in our vocabulary are weighted evenely, that is to say that matching 'are' is the same as matching 'radio'. Intuitevly, we know that matching 'radio' is much more likely to narrow down the results in our favor vs matching 'are. How do we capture this idea mathematically? \n",
    "First we must have a definition of what an 'important' word is. Perhaps the more common a word is, the less important? This is the idea behind the TF-IDF (term frequency, inverse-document frequency) algorithm. A word's importance in a document is a balance between the amount of times it appears in the document and the amount of times it appears in the entire corpus. Each word in the vocabulary is given a TF-IDF weight, and this weight can be used to filter out words that aren't very helpful.\n",
    "\n",
    "For example, if you're corpus is a collection of news reports from the latest PGA golf tournament, the word 'golf' is not really a word that would allow us to differentiate documents well. Even though the word 'golf' might appear in a particular document very often (increasing the TF-IDF weighting), it also appears in the corpus very often (lowering the TF-IDF weighting). Compare this to a word like 'president', which may occur very frequentely in one document (maybe about the president of the tournament) but rarely in other documents, this word will have a high TF-IDF weight. \n",
    "\n",
    "The actual TF-IDF formula is below:\n",
    "\n",
    "\n",
    "\n",
    "Let's write a program to calculate the TF-IDF weight for all the terms in our Vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0]*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before wrapping up this guide, we discuss yet another shortcoming of the Vector Space Model and how to fix it. We observed above that matching certain words can be more important then matching others, however what about matching the same word multiple times? Why might this become an issue in certain contexts? \n",
    "Suppose we're running a search engine, a spammer who is trying to get his website ranked first by our search engine could just construct hundreds of web pages that repeat particular uncommon words thousands of times, ensuring that whenever someone searches for one of these terms they get routed to his website. Certain documents might also contain repeated mentions of a word but from a different context, so while there are not many words matched overall the repeat of the particular word drives up the TF-IDF Score. \n",
    "One way we could address this issue is to not reward every occurance of a word equally. The first few occurances are important, but the 100th or 1000th occurance perhaps not so much. We need something like the BM25 transformation.\n",
    "\n",
    "Let x = # of times a word appears in a document\n",
    "Let x' = (x+1)k/x+k\n",
    "\n",
    "Now, depending on the value of k we pick, the value of x' diminishes as the mentions of a word increase. The contribution is still positive for each word. Here's the graph of x' vs x.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Well, the number of times we may see a word in a document is influenced by how long the document itself is! If one of our documents is thousands of pages long, it has many more 'chances' to match words from the source document. It may even match one of the words from the vocabulary as many times as a more targeted and relevant document. We need some way to penalize multiple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector(\"There was a fox fur hunt going on\", vocabulary=Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
