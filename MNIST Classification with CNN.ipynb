{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the MNIST (handwritten digits) dataset using Covolutional Neural Network\n",
    "### Test accuracy > 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defning network and training\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = Adam()\n",
    "N_HIDDEN = 250\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "10477568/11490434 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "## Data Splitting\n",
    "(X_train,Y_train),(X_test,Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train is a column vector where each row is a 28x28 array (60k rows)\n",
    "# Let's reshape this so that each entry in the array is a column\n",
    "# 60k x 784\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since these are pixel values, we should normalize by color channel (255)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the outputs are classes, we should convert to catagorical\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 250)               196250    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                2510      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 198,760\n",
      "Trainable params: 198,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Network Construction\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED, ))) # Adding a dense layer, with 10 neurons (1 for each class)\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax')) # Adding a softmax activation function for multi-class output\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "48000/48000 [==============================] - 3s - loss: 0.0203 - acc: 0.9940 - val_loss: 0.0729 - val_acc: 0.9801\n",
      "Epoch 2/30\n",
      "48000/48000 [==============================] - 3s - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0714 - val_acc: 0.9802\n",
      "Epoch 3/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0189 - acc: 0.9941 - val_loss: 0.0736 - val_acc: 0.9805\n",
      "Epoch 4/30\n",
      "48000/48000 [==============================] - 3s - loss: 0.0178 - acc: 0.9939 - val_loss: 0.0729 - val_acc: 0.9805\n",
      "Epoch 5/30\n",
      "48000/48000 [==============================] - 3s - loss: 0.0178 - acc: 0.9941 - val_loss: 0.0720 - val_acc: 0.9802\n",
      "Epoch 6/30\n",
      "48000/48000 [==============================] - 3s - loss: 0.0181 - acc: 0.9939 - val_loss: 0.0760 - val_acc: 0.9802\n",
      "Epoch 7/30\n",
      "48000/48000 [==============================] - 3s - loss: 0.0164 - acc: 0.9949 - val_loss: 0.0819 - val_acc: 0.9803\n",
      "Epoch 8/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0158 - acc: 0.9947 - val_loss: 0.0748 - val_acc: 0.9809\n",
      "Epoch 9/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0141 - acc: 0.9953 - val_loss: 0.0752 - val_acc: 0.9800\n",
      "Epoch 10/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0136 - acc: 0.9953 - val_loss: 0.0781 - val_acc: 0.9802\n",
      "Epoch 11/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0138 - acc: 0.9952 - val_loss: 0.0816 - val_acc: 0.9801\n",
      "Epoch 12/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0148 - acc: 0.9948 - val_loss: 0.0791 - val_acc: 0.9807\n",
      "Epoch 13/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0132 - acc: 0.9952 - val_loss: 0.0793 - val_acc: 0.9806\n",
      "Epoch 14/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0117 - acc: 0.9960 - val_loss: 0.0804 - val_acc: 0.9808\n",
      "Epoch 15/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0130 - acc: 0.9958 - val_loss: 0.0812 - val_acc: 0.9816\n",
      "Epoch 16/30\n",
      "48000/48000 [==============================] - 4s - loss: 0.0132 - acc: 0.9956 - val_loss: 0.0794 - val_acc: 0.9818\n",
      "Epoch 17/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0110 - acc: 0.9966 - val_loss: 0.0825 - val_acc: 0.9805\n",
      "Epoch 18/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0820 - val_acc: 0.9806\n",
      "Epoch 19/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0116 - acc: 0.9964 - val_loss: 0.0871 - val_acc: 0.9802\n",
      "Epoch 20/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0108 - acc: 0.9963 - val_loss: 0.0839 - val_acc: 0.9804\n",
      "Epoch 21/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0102 - acc: 0.9968 - val_loss: 0.0866 - val_acc: 0.9807\n",
      "Epoch 22/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0122 - acc: 0.9961 - val_loss: 0.0837 - val_acc: 0.9827\n",
      "Epoch 23/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0868 - val_acc: 0.9825\n",
      "Epoch 24/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0105 - acc: 0.9963 - val_loss: 0.0863 - val_acc: 0.9817\n",
      "Epoch 25/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0110 - acc: 0.9963 - val_loss: 0.0908 - val_acc: 0.9806\n",
      "Epoch 26/30\n",
      "48000/48000 [==============================] - 5s - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0896 - val_acc: 0.9815\n",
      "Epoch 27/30\n",
      "48000/48000 [==============================] - 6s - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0892 - val_acc: 0.9822\n",
      "Epoch 28/30\n",
      "48000/48000 [==============================] - 6s - loss: 0.0095 - acc: 0.9968 - val_loss: 0.0846 - val_acc: 0.9821\n",
      "Epoch 29/30\n",
      "48000/48000 [==============================] - 6s - loss: 0.0086 - acc: 0.9968 - val_loss: 0.0849 - val_acc: 0.9829\n",
      "Epoch 30/30\n",
      "48000/48000 [==============================] - 6s - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0840 - val_acc: 0.9812\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit(X_train, Y_train, batch_size=128, \n",
    "                    epochs=30, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.0734901695074\n",
      "Test accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "## Scoring\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CONVULUTIONAL DEEP NETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defning network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "IMG_ROWS, IMG_COLS = 28,28\n",
    "NB_CLASSES = 10\n",
    "INPUT_SHAPE = (1,IMG_ROWS,IMG_COLS)\n",
    "OPTIMIZER = Adam()\n",
    "N_HIDDEN = 250\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD,RMSprop, Adam\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the convNet\n",
    "class LeNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes):\n",
    "        model = Sequential()\n",
    "        # CONV -> RELU -> POOL\n",
    "        model.add(Conv2D(50, kernel_size=5,padding='same', input_shape=input_shape))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "        model.add(Conv2D(50,kernel_size = 5, border_mode='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "        model.add(Activation('relu'))\n",
    "        # Flatten => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        # Softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and formatting data\n",
    "(X_train, y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "# X_train is 60,000 arrays of size 28x28\n",
    "print(y_train.shape)\n",
    "# y_train is 60,000 integer lables\n",
    "\n",
    "\n",
    "K.set_image_dim_ordering(\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to float and normalize\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Need a 60k x [1x28x28] shape as input to the convnet\n",
    "X_train = X_train[:, np.newaxis, :, :]\n",
    "X_test = X_test[:,np.newaxis, :, :]\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "print(y_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(50, padding=\"same\", kernel_size=5)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Init the optimizer and model\n",
    "model = LeNet.build(input_shape = INPUT_SHAPE, classes = NB_CLASSES)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 751s - loss: 0.1420 - acc: 0.9551 - val_loss: 0.0508 - val_acc: 0.9852\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 818s - loss: 0.0390 - acc: 0.9877 - val_loss: 0.0409 - val_acc: 0.9888\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 682s - loss: 0.0272 - acc: 0.9909 - val_loss: 0.0361 - val_acc: 0.9890\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 673s - loss: 0.0182 - acc: 0.9939 - val_loss: 0.0442 - val_acc: 0.9883\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 667s - loss: 0.0154 - acc: 0.9948 - val_loss: 0.0344 - val_acc: 0.9900\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 664s - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0398 - val_acc: 0.9902\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 667s - loss: 0.0093 - acc: 0.9967 - val_loss: 0.0325 - val_acc: 0.9911\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 667s - loss: 0.0070 - acc: 0.9975 - val_loss: 0.0345 - val_acc: 0.9919\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 670s - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0523 - val_acc: 0.9870\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 911s - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0383 - val_acc: 0.9908\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 890s - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0424 - val_acc: 0.9901\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 714s - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0427 - val_acc: 0.9907\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 689s - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0356 - val_acc: 0.9920\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 683s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0398 - val_acc: 0.9922\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 691s - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0774 - val_acc: 0.9848\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 681s - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0420 - val_acc: 0.9903\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 681s - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0451 - val_acc: 0.9917\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 675s - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0393 - val_acc: 0.9918\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 678s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0433 - val_acc: 0.9915\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 674s - loss: 7.3491e-04 - acc: 0.9998 - val_loss: 0.0472 - val_acc: 0.9917\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs = NB_EPOCH, verbose=VERBOSE, \n",
    "                   validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s    \n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.0446791687175\n",
      "Test Accuracy:  0.9907\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score: \", score[0])\n",
    "print(\"Test Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
