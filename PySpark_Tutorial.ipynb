{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png) \n",
    "# **Machine Learning with Apache Spark and Python**\n",
    "\n",
    "This notebook will cover the major aspects of performing analytics on large datasets using the distributed computing platform Spark and the programming langauge Python. \n",
    "\n",
    "** Major topics covered in this tutorial include: **\n",
    "* Basic Data Wrangling and Manipulation\n",
    "* User-defined Functions\n",
    "* Basic Plotting\n",
    "* Distributing Analytics Tasks\n",
    "* Best Practices\n",
    "* Use Case -- Time-Series Indexing with iSax\n",
    "* Use Case -- Sensory Anomaly Detection\n",
    "* Use Case -- Genetic Algorithms for Optimization\n",
    "* Performace Comparisons\n",
    "* Additional Resources\n",
    "\n",
    "Note that for reference, you can look up the details of the relevant methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Platform and Dataset**\n",
    "\n",
    "This tutorial was constructed using a iPython/Jupyter notebook hosted on the Databricks platform. Databricks offers a free community edition which provides all the functionality to run this notebook with the included dataset. Otherwise, the notebook is constructed in a logical fashion with all output included, and can be replicated on any platform that supports PySpark or simply read through. \n",
    "\n",
    "The dataset used for this tutorial represents some basic parameters for two seperate flights, including flight number, time, phase, n1 (fan speed), and engine_position. The identifiers and parameters have been randomized but the data still represents somewhat typical measurements for these parameters on a flight. The dataset contains about half a million rows, with varying levels of completion in each of the parameters. While this dataset is not excessively large in nature, it is large enough to see performance increases from distribution and to illustrate the major concepts of Spark. \n",
    "\n",
    "Using the sqlContext and the SQL langauge, we can call our table into an DataFrame data structure and then preview the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sqlContext and SparkContext need to be defined on certain distributions, such as below on a HDFS-Hive system\n",
    "# sc = SparkContext(conf=conf)\n",
    "# sqlContext = HiveContext(sc)\n",
    "\n",
    "\n",
    "\n",
    "dataDF = sqlContext.sql(\"SELECT * FROM flt_data_randomized ORDER BY frame_offset\")\n",
    "dataDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The RDD Data Structure and Lazy Loading**\n",
    "\n",
    "The base data structure in Spark is the RDD (Resiliant Distributed Dataset). While this data strcture contains all the elements of your dataset, those elements are distributed across various nodes in the cluster. It is possible to collect the data structure back in an ordered format via the collect() method, but this involves aggregating all the data onto the main node and is not recomended for very large datasets. Traditional ordered row operations are also harder to apply, but can be done via the map() function, which we will use later in this tutorial. While the RDD class contains many operations similar to traditional data frames from Python and R, there is a newer DataFrame data structure available to Spark that more closely mimics the traditional data frame strucutre. We will alternate between using both of these data structures for the remainder of this tutorial, as they're both suited better for different tasks.\n",
    "\n",
    "\n",
    "Keep in mind also that many operations are simply 'remembered' in memory and not actually performed unless some output needs to be given. This is called 'lazy evaluation', from the Spark webpage: \"All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Wrangling</h1>\n",
    "\n",
    "\n",
    "\n",
    "For the purposes of data splicing, the DataFrame is a better data structure to use, luckily our data was imported as a DataFrame when we used the SQL command above. To select particular columns from the data we can use the select() method, this results in a new DataFrame with only the listed columns. To collect and view our DataFrame we need the show() command, remember show() collects all the data onto the main node. Below we will select columns for the flight number, time measurement (frame_offset), and the fan speed measurements (n1). We then order the data according to the frame_offset, since it is a time measurement.\n",
    "\n",
    "![Data Wrangling](http://alumni.berkeley.edu/sites/default/files/styles/960x400/public/wranglingbigdata.jpg?itok=k0fK1fJQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF2 = dataDF.select('adi_flight_record_number', 'frame_offset', 'n1')\n",
    "dataDF2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the output above, we see that there is a lot of missing n1 measurements, we'd like to exclude these from our analysis. For this we can use the filter() method, which will allow us to set a condition on which to filter our data. These filters can be chained. Since there are four observations with a 0 frame_offset, we will remove these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF2 = dataDF2.filter(dataDF2.n1 > 0)\n",
    "dataDF2.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Column Creation and Transformation</h1>\n",
    "\n",
    "Column operations in Spark have a straight-forward syntax, and we can either transform an existing column or create a new one from an existing column. This is all done via the withColumn() method. The first argument passed will be the name of the column to replace (if a name is given that does not correspond to an existing one, a new column with that name will be created). Assume we find out our time measurements actually begin 2 seconds after the flight begins, and thus we add a 2 second delay to our frame_offset column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF2 = dataDF2.withColumn('frame_offset', dataDF2.frame_offset + 2)\n",
    "dataDF2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get basic column statistics by using the describe() method, calling this on a DataFrame with multiple columns will give you summary statistics on each of the columns. To make it interesting, we will filter out the first 30 seconds of the flight and see the average fan speed (n1). \n",
    "\n",
    "\n",
    "\\* If the data was imported so that the 'n1' column is not a float type, then you will also need to run the commented code to get it in the proper format for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF2 = dataDF2.select('adi_flight_record_number', 'frame_offset',dataDF2.n1.cast('float')) # Type Conversion Code\n",
    "\n",
    "\n",
    "dataDF2.filter(dataDF2.frame_offset <= 30).describe().show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since describe() can be display several outputs at once, we will compare the first 30 seconds of fan speed to the first 10 minutes into the flight. Since fan speed ramps up during the takeoff phase, we expect the second window of time to have a higher mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF2.filter(dataDF2.frame_offset <= 30).select('n1').describe().show() # Code from above, first 30 seconds of flight\n",
    "\n",
    "dataDF2.filter(dataDF2.frame_offset <= (10*60)).select('n1').describe().show() # First 10 minutes of flight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GroupBy and Aggregate Functions**\n",
    "\n",
    "![groupBy](https://silviomassari.files.wordpress.com/2011/07/mapreducetwo.png)\n",
    "\n",
    "In Spark, groupBy() is one of the most powerful transformations. It allows you to perform aggregations on a DataFrame. Unlike other DataFrame transformations, groupBy() does not return a DataFrame. Instead, it returns a special GroupedData object that contains various aggregation functions. The most commonly used aggregation function is count(), but there are others (like sum(), max(), and avg(). These aggregation functions typically create a new column and return a new DataFrame.\n",
    "\n",
    "Since we have two seperate flights in our data, let's group our observations by flight number and count how many observations are in each group using the aggregate function count(). We will be using these flight numbers as keys very soon. Since the result of an aggregate function is a DataFrame, we must use show() to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF2.groupBy('adi_flight_record_number').count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>User-Defined Functions</h1>\n",
    "\n",
    "Spark has the ability to define a function and use it on a column as well, by registering the function as something called a 'User-Defined Function' (UDF), the syntax for this is not very difficult, although your function needs to be registered as a UDF or applied via the [lambda syntax](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.functions.udf). Here we will use the output of our 'describe()' function from a few cells above to normalize our data via a User-Defined Function. Syntax is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define udf\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def Znormalize(value):\n",
    "    mean = 82.82697\n",
    "    sd = 23.34478\n",
    "    return ((value - mean)/sd)\n",
    " \n",
    "udf_Znormalize = udf(Znormalize)\n",
    "\n",
    "dataDF2.withColumn(\"n1\", udf_Znormalize(\"n1\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Row Operations </h1>\n",
    "\n",
    "We've observed that we can do column operations using the withColumn() method, now let's understand how row operations are done in Spark. For a traditional DataFrame, row operations can be done via the map() method. The syntax for this is similar in flavor to the withColumn syntax, except no row specification is taken so some slicing must be done beforehand if only operating on a subset of rows. We will meet the sister function mapValues() later on in this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Plotting</h1>\n",
    "\n",
    "**Basic Example**\n",
    "\n",
    "Now, since we know how to splice our data, manipulate it, and get basic descriptive statistics the next logical step is visual. We'd like to generate a readable and visually interesting depiction of patterns in our data. Since we have access to engine fan speed and time, our options are somewhat limited but we can still generate a pretty interesting plot. We will be using the Pandas and Matplotlib libraries, these should be automatically installed into the Databricks environment so we simply need to import them. We then clear the figure in case a previous one is stored, send a basic line graph to the canvas, add vertical lines to seperate flight phases, add labels and title, then finish off by displaying the canvas.\n",
    "\n",
    "At the time of writing, there is not a way to plot in a distributed fashion; the data must be collected onto the main node, converted to a Pandas dataframe, and then plotted via the matplotlib library. This is not a huge concern, since only a small well-chosen subset of data should be used to plot so readability is not hindered.\n",
    "\n",
    "Drawing a different kind of graph can be as simple as changing the 'kind' parameter in the plot() call, see the comprehensive (and very readable) PyPlot documentation at [http://matplotlib.org/users/pyplot_tutorial.html] \n",
    "\n",
    "\\* Note: On certain clusters, a canvas may for display() may need to be created, this is beyond the scope of this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.clf() # Clearing the canvas for a new plot\n",
    "\n",
    "# Converting the Spark DataFrame to a Pandas Dataframe and making sure n1 is a float type\n",
    "pdf = dataDF2.toPandas()\n",
    "pdf.n1 = pdf.n1.astype(float)\n",
    "\n",
    "# Constructing the plot\n",
    "pdf.frame_offset = pdf.frame_offset.astype(float)\n",
    "pdf.plot(x = 'frame_offset', y = 'n1', kind = 'line', color = 'blue', linewidth = 0.010)\n",
    "\n",
    "\n",
    "# draw vertical line showing flight phases\n",
    "plt.plot([2000, 2000], [0, 100], 'k-', lw=2, color='red') # [X2, X1], [Y1,Y2]\n",
    "plt.plot([9000, 9000], [0, 100], 'k-', lw=2, color='red') # [X2, X1], [Y1,Y2]\n",
    "plt.ylabel(\"Max N1 (fan speed)\")\n",
    "plt.xlabel(\"Frame Offset\")\n",
    "plt.title(\"Time vs. Fan Speed (N1) For a Single Flight\")\n",
    "\n",
    "\n",
    "\n",
    "display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Example**\n",
    "\n",
    "Often, there are times where it is neccesary to integrate some basic analytic views into our plots. Since our plots are generated using the Matplotlib library, they are very compatible with Pandas objects of many types, and we can use numpy or scipy to operate on these Pandas objects. Below, we use a process called [Moving Average](https://en.wikipedia.org/wiki/Moving_average) to smooth the fan speed values for the first 50 seconds of the flight 'enrout' or cruise phase. We then calculate the standard deviation of these values. Finally, we make a plot with the original values (with transperency) overlaid with the moving average and a 'ribbon' for 2 standard deviations (since values >2 standard deviations may need to be examined for outliers). We make a function out of the entire process to make it re-usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.clf()\n",
    "#\n",
    "dataDF = dataDF.filter(dataDF.flight_phase != 'null')\n",
    "dataDF.select('flight_phase').distinct().show()\n",
    "dataDF_Enrout = dataDF.filter(dataDF.flight_phase == 'G_ENROUT')\n",
    "#\n",
    "Enrout_Pandas = dataDF_Enrout.select('n1').toPandas()\n",
    "\n",
    "def ribbonPlot(values, period = 20):\n",
    "  price = pd.Series(values)\n",
    "  ma = pd.rolling_mean(price, period)\n",
    "  mstd = pd.rolling_std(price, period)\n",
    "  plt.figure()\n",
    "  plt.plot(price.index, price, 'k', alpha = 0.2)\n",
    "  plt.plot(ma.index, ma, 'b')\n",
    "  plt.fill_between(mstd.index, ma-2*mstd, ma+2*mstd, color='red', alpha=0.5)\n",
    "  \n",
    " \n",
    "\n",
    "\n",
    "  blue_line = mlines.Line2D([], [], color='blue', marker='',markersize=15, label='Moving Average (4)')\n",
    "  black_line = mlines.Line2D([], [], color = 'black', markersize = 15, label = 'Values', alpha = 0.2)\n",
    "  red_line = mlines.Line2D([],[], color = 'red', markersize = 15, label = '2 Std.Deviations', alpha = 0.5)\n",
    "  plt.legend(handles=[black_line, blue_line, red_line])\n",
    "\n",
    "  \n",
    "  plt.ylabel('Fan Speed (n1)')\n",
    "  plt.xlabel('Time into Phase (seconds)')\n",
    "  plt.title('Time vs. Fan Speed (N1) For Enrout Phase')\n",
    "    \n",
    "  display()\n",
    "  \n",
    "ribbonPlot(Enrout_Pandas['n1'][1:50], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Writing Distributed Functions </h1>\n",
    "\n",
    "**Data Prep**\n",
    "\n",
    "The main use-case for Spark often becomes the ability to run algorithms on large datasets in parallel. The computation time difference between a simple loop and parallel loop grows hugely as data sources become larger, so the understanding of how we can define and run functions that take advantage of distribution is paramount. The first thing to understand before we begain writing distributed functions is an understanding on how to seperate the data into chunks, each of which can run our algorithm seperately. The illustration below (courtesy of Mathworks and Matlab) sums up the process and comparison nicely.\n",
    "\n",
    "![Distributed Computing](http://www.mathworks.com/cmsimages/63635_wl_91710v00_po_fig2_wl.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Pair RDD**\n",
    "\n",
    "This task of seperating data into computation chunks lends itself naturally to the idea of a key-value pair. Since each of the keys should contain data that is independent of the other keys, this means that splitting our computation between key-value pairs should not affect the structure of the computation. In our example, we have two seperate flight numbers contained within our data, each of which has it's own set of frame_offset and n1 values as well as time measurements. We will use these flight numbers as our keys, and the parameters frame_offset and n1 as our values. The best way to contain such an arrangement is the Pair RDD data structure, to create it we must split the data in a very precise manner.\n",
    "\n",
    "The construction of pair RDD relies on data in the form of (key (value1, value2 ...)) where the key-value pair is contained in a tuple and the set of values for a key is contained in a tuple. The code below uses the map() command, which iterates through the rows of the DataFrame (including key and values), then uses a temporary lambda function to re-format each row into the proper format. \n",
    "\n",
    "The result is an object of type 'PipelinedRDD' (a special case of the RDD object discussed earlier), and thus we must use the collect() method (analogus to show() on DataFrames) to give a view of the result. Here we use the first column of our DataFrame (adi_flight_record_number) as our key, then the second (frame_offset/time) and third (n1/fan speed) columns as our values. It is not neccesary to convert the key to a string, but it may avoid some mistakes in the future.\n",
    "\n",
    "\\* REMEMBER, Python list splicing is non-inclusive to the endpoint. 1:3 only reports the index 1 and index 2 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF3 = dataDF2.map(lambda x: (str(x[0]),list(x[1:3])))\n",
    "dataDF3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is now in the proper form, we are ready to convert it into the Pair RDD data structure. We first use the groupByKey() function which simply groups the data above into groups by flight number (two groups, in our case). The result of the groupBy is a special data form that is very conductive to parallel computing, as a tuple of key-value pairs where the values are stored as a 'resultIterable' object. This resultIterable object is just what it seems, it's an object that Spark can iteratively apply a function through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF3 = dataDF3.groupByKey()\n",
    "dataDF3.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips for Distributed Functions**\n",
    "\n",
    "The key to writing good distributed functions is an understanding of what information is lost when we seperate the data by keys, and a strong comprehension of the contents of the resultIterable object seen above. For our case, not much information is lost at all while converting to key-value pairs, since each key corresponds to a full flight's worth of data. However, looking at fleet or customer-wide trends becomes significantly harder in this paradigm, and either clever keys need to be created or otherwise clever ways of joining the results of many computations (more on this later). The contents of our resultIterable object is a tuple of lists; that is to say each element of resultIterable is of the form [frame_offset, n1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loops and Groups**\n",
    "\n",
    "Now that we have an understanding of the nature of the data split and the contents of each split, let's start constructing our function. An easy way to guide your function design is with the idea of 'loops and groups', that is to say any function should contain a loop of some sort and only operate on one group of the data. Below is a simple function that loops through each resultIterable and gives us the sum of the n1 values, where the groups are flights (there are two in our example). Remember that each element is itself a list [frame_offset, n1] so we only want to loop through the second (1 in Python) index of each element. \n",
    "\n",
    "The function must be passed via the lambda notation, and is applied via the mapValues() method since we are only operating on the values and not the keys. Note that this function will be 'sent' to each group and run *AT THE SAME TIME*, with each group reporting its key and the output of its function application. Here lies the benefit of parallel computing, scaling this to a hundred or even a thousand flights can be done much more quickly and naturally then if we were to iterate through each flight in order. \n",
    "\n",
    "Below we construct a simple function that returns the sum of the values passed to it for each given key (all computed in parallel). Note the function only adds the second column (hence the 1 indexing) because the values are a list of the format [frame_offset, n1] and we only want to get the sum of the n1 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSum(iterableObj):\n",
    "  ''' This function will return the \n",
    "       sum of the numeric elements of \n",
    "       the iterable object passed to it'''\n",
    "  \n",
    "  sum = 0\n",
    "  for i in iterableObj:\n",
    "    sum += i[1]\n",
    "  return(sum)\n",
    "\n",
    "dataDF3.mapValues(lambda x: getSum(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Best Practices </h1>\n",
    "\n",
    "**The Importance of Error Handling**\n",
    "\n",
    "*try-catch* : \n",
    "Since parallel functions often deal with giant volumes of data, it is often very difficult to get a feel for the types of data issues that may arise. Also since these functions process such large amounts of data an error can wreak havoc on functionality of the entire script. Thus, general exception handling and 'defensive' coding practices become very important. The simplest and most essential mechanism for handling this in Python is via the try,catch statements. These statements can handle [exceptions](http://www.tutorialspoint.com/python/python_exceptions.htm) that occur inside the 'try' block and perform an operation specified in the 'catch' block. In the frequantly occuring case of TypeErrors, we can simply print the element that is not of proper type and then skip it in the operation, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSum(iterableObj):\n",
    "  ''' This function will return the \n",
    "       sum of the numeric elements of \n",
    "       the iterable object passed to it'''\n",
    "  \n",
    "  sum = 0\n",
    "  for i in iterableObj:\n",
    "    try:\n",
    "      sum += i\n",
    "    except TypeError as t:\n",
    "      print(str(i) + \" is a \" + str(type(i)) + \", not a numeric type\")\n",
    "      pass\n",
    "  return(sum)\n",
    "\n",
    "badList = [1,4,5, 'notANumber', 4]\n",
    "\n",
    "getSum(badList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error-handling via If-Else**:\n",
    "\n",
    "There is often confusion between using If-Else or Try-Catch to handle an exception that may occur, and often the two can become interchangable. See the code below, its output will be the same as the one above however it is implemented using an if-else to handle incorrect types. Which one should be used? In this case, the If-Else is a better option, simply because TypeErrors may be commonplace with large amounts of data (so not really an exceptional case) and throwing exceptions can be somewhat more computationally intensive then simply avoiding the case with If-Else. You will notice that we had to check over multiple numeric types in our If condition, this is another consideration when choosing your exception handling mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSum2(iterableObj):\n",
    "  ''' This function will return the \n",
    "       sum of the numeric elements of \n",
    "       the iterable object passed to it'''\n",
    "  \n",
    "  sum = 0\n",
    "  for i in iterableObj:\n",
    "    if(isinstance(i, (int, long, float, complex))):\n",
    "      sum += i\n",
    "    else:\n",
    "      print(str(i) + \" is a \" + str(type(i)) + \", not a numeric type\")\n",
    "      pass\n",
    "  return(sum)\n",
    "\n",
    "badList = [1,4,5, 'notANumber', 4]\n",
    "\n",
    "getSum2(badList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avoiding Data Shuffle**\n",
    "\n",
    "Using reduceByKey() vs groupByKey(): There is a large discussion in the Spark community about replacing groupByKey() with other methods such as reduceByKey() or combineByKey(), this is mainly because groupByKey() causes a lot of data to be 'shuffled' around the network in the grouping process, and when there are additonal operations being performed this can lead to ineffeciancy or even memory-overload. Explaining the computational differences between the two is beyond the scope of this tutorial, but it is better to use reduceByKey() when the operation can be formatted in a reduction sense (many operations cannot be formatted in this way or become excessively complicated). See this [post](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html) for a full explanation\n",
    "\n",
    "Not doing collect() or show(): For large datasets the collect() or show() command should not be used prior to seperating a managable chunk of data out, these commands gather all the data they're called upon to the main node, and may cause memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>**Use Case: iSax Indexing for Time Series**</h1>\n",
    "\n",
    "The type of data we encounter in this tutorial is an instance of something called a time-series, or a series of (x,y) values where the x values are time measurements. The y values are measurements of fan speed (n1) in our case, but they can be any value we wish to observe and draw conclusions about. A lot of traditional parametric statistical techniques do not work well on time-series data, since time-based data has varying distributions and is often non-normal. The usage of time-series specific algorithms and models can become very computationally complex and are also preceeded by methods checking the nature of the data and its suceptibility to certain kinds of models. Thus, taking the problem of comparing two time-series out of the time-series domain and into a more computationally simple paradigm can be of great benefit. This paper introduces the concept of converting a time series into a sequence of letters, with each time series constituting a 'word' as a whole. Comparing two time-series becomes a matter of comparing their words, a much simpler text analysis and matching problem.\n",
    "![iSax](http://images.slideplayer.com/21/6323537/slides/slide_61.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Writing the iSax function for an individual Time-Series**\n",
    "\n",
    "The function defined below will take two parameters, timeList (time parameter) and valueList (value parameter), and returns the iSax 'word' for the corresponding time series. There are many aspects of the algorithm below, the general method is outlined in this [research paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.439.3743&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iSax(timeList, valueList, time_bucket_length = 14, alphabet_length = 4, max_vector = 110, min_vector = 0):\n",
    "  \n",
    "  '''\n",
    "  This function implements the iSax indexing for a single variable time-series\n",
    "  \n",
    "  Input 1: timeList -- An ordered list of time observations from the time series\n",
    "  Input 2: valueList -- An ordered list of value observations from the time series\n",
    "  Input 3: time_bucket_length -- The time interval that each bucket/letter should be constructed of\n",
    "  Input 4: alphabet_length -- The length of the overall word\n",
    "  Input 5: max_vector -- Max value\n",
    "  Input 6: min_vector -- Min value\n",
    "  \n",
    "  Returns: word -- The iSax 'word' corresponding to the input time series\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  ## Parameter Definitions\n",
    "  time_bucket_length = 14 # Length of each time bucket\n",
    "  alphabet_length = 4 # Length of overall output\n",
    "  max_vector = 110 \n",
    "  min_vector = 0\n",
    "  \n",
    "  length_of_ts = len(timeList)\n",
    "  index_pairs = []\n",
    "  \n",
    "  current_time = time_bucket_length\n",
    "  temp_start = 0\n",
    "  temp_end = 0\n",
    "  \n",
    "  endTime = 0\n",
    "  for j in timeList:\n",
    "    if j>endTime:\n",
    "      endTime = j\n",
    "  \n",
    "  \n",
    "  while current_time < (endTime + time_bucket_length): # While we haven't reached the end of our time series\n",
    "    bucket = []\n",
    "    index = 0\n",
    "    for item in timeList:\n",
    "      if (item <= current_time) and (item > (current_time - time_bucket_length)): # For time values in our current interval\n",
    "        bucket.append(index) # Get the indexes of all time values in our current interval\n",
    "      index += 1\n",
    "    if len(bucket) == 0:\n",
    "      # empty bucket - insert null pair\n",
    "      index_pairs.append('blank')\n",
    "    elif len(bucket) == 1:\n",
    "      # only 1 value in bucket - time slice 1 value\n",
    "      index_pairs.append((bucket[0], bucket[0] + 1))\n",
    "    else:\n",
    "      index_pairs.append((bucket[0], bucket[-1] + 1))\n",
    "    current_time += time_bucket_length\n",
    "  \n",
    "\n",
    "  word = []\n",
    "  for item in index_pairs:\n",
    "    \n",
    "  \n",
    "    \n",
    "    if item == 'blank':\n",
    "      mean = min_vector   \n",
    "      \n",
    "    elif (item[1]-item[0])==1: # If there's only one value in range, make mean the one value\n",
    "      mean = valueList[item[0]]\n",
    "      \n",
    "    else: # For multiple values, actually take mean of values\n",
    "      temp_list = valueList[item[0]:item[1]]\n",
    "      count = 0\n",
    "      total = 0\n",
    "      for value in temp_list:\n",
    "        if value is None: # Data from flight could cause value to be None, causes type mismatch error at total+=value\n",
    "          pass\n",
    "        else:\n",
    "          count += 1\n",
    "          total += value\n",
    "      mean = total/count\n",
    "      \n",
    "      \n",
    "      \n",
    "    vector_bucket_len = float(max_vector - min_vector) / float(alphabet_length)\n",
    "    index = 0\n",
    "    check_value = float(min_vector)+ float(vector_bucket_len)\n",
    "      \n",
    "      \n",
    "    for i in range(alphabet_length):\n",
    "      if mean < check_value:\n",
    "        word.append(chr(65+index))\n",
    "        break\n",
    "      check_value += vector_bucket_len\n",
    "      index += 1\n",
    "      \n",
    "  word = ''.join(word)\n",
    "  \n",
    "  return(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying iSax via loop list creation**\n",
    "\n",
    "Since the inputs to the iSax function need to be lists, it is more conveniant for us to loop through the rows of our dataset (now a resultIterable and seperated by flight number) and collect the columns back as lists. These columns can then be passed to iSax and the result returned, all in a distributed fashion via the mapValues() function. Notice how the output has a long string of B's, with more variation towards the tails. These is natural as it correspons to flight phases such as accelration/takeoff, cruise (longest phase), and descent/landing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i[0] is frame offset\n",
    "# i[1] is Engine 1\n",
    "\n",
    "\n",
    "from operator import *\n",
    "\n",
    "def getiSax(iterableObj):\n",
    "  \n",
    "  '''\n",
    "  This function takes an resultIterable object (such as the values from a pair RDD) and constructs lists of the values. Then,\n",
    "  as defined in the function block below, it operates on those lists. When a function is passed through the Lambda function to mapValues, it distributes the function\n",
    "  to each of the keys in the RDD. \n",
    "  \n",
    "  The commented our portions are to increase functionality in seperating results be engine, in which case we would need to include the engine1 and engine2 columns in place\n",
    "  of n1 in the Pair RDD creation above\n",
    "  '''\n",
    "  ### Placeholder Lists\n",
    "  frame_offset_list = []\n",
    "  engine1_n1 = []\n",
    "  #engine2_n1 = []\n",
    "  \n",
    "  ### Main Loop\n",
    "  for i in iterableObj:\n",
    "    (frame_offset_list.append(i[0]))\n",
    "    (engine1_n1.append(i[1]))\n",
    "    #(engine2_n1.append(i[2]))\n",
    " \n",
    "  \n",
    "  ### Functional Block\n",
    "  iSax_Strings = iSax(frame_offset_list, engine1_n1)\n",
    "  ### \n",
    "  \n",
    "  return(iSax_Strings)\n",
    "\n",
    "  \n",
    "      \n",
    "  \n",
    "\n",
    "dataDF3.mapValues(lambda x: getiSax(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison and Matching using iSax**\n",
    "\n",
    "We can use the iSax-indexed flights to isolate certain concerning patterns or to compare a parameter (n1 in this case) between two flights. We can use the [Levenshtein Distance Metric](https://en.wikipedia.org/wiki/Levenshtein_distance) for strings to compare the two flights and get a 'difference' score. In the future we can build a distribution for these difference scores and examine the flights on the tail ends of this distribution.\n",
    "Examining flights where part failure or otherwise erroneous performance occured, we can see the string patterns that preceeded the failure and try to detect these issues in current flights. Suppose the pattern \"BAA\" was detected for a N1 parameter on a flight right before there was a fan blade fracture. We can attempt to find this pattern in our current flight and perform maintainence before a further failure occurs. \n",
    "\n",
    "The first output below is the Levenshtein Distance between the two flights, and the second output is the index in flight1 where the pattern \"BAA\" appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LevDistance(str1, str2):\n",
    "  edits = 0\n",
    "  for i, j in zip(list(str1), list(str2)):\n",
    "    if i!= j:\n",
    "      edits += 1\n",
    "  return(edits)\n",
    "\n",
    "indexedSeries = dataDF3.mapValues(lambda x: getiSax(x)).collect()\n",
    "flight1_word = indexedSeries[0][1]\n",
    "flight2_word = indexedSeries[1][1]\n",
    "\n",
    "print(\"Levenshtein Distance = \" + str(LevDistance(flight1_word, flight2_word)))\n",
    "\n",
    "pattern = \"BAA\"\n",
    "foundIndex = flight1_word.find(pattern)\n",
    "\n",
    "if foundIndex > 0:\n",
    "  print(\"Pattern \" + pattern + \" found at index \" + str(foundIndex))\n",
    "else:\n",
    "  print(\"Pattern not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Use Case - Sensory Anomaly Detection </h1>\n",
    "\n",
    "Most aircraft will have more then one engine aboard, and since each engine is equipped with its own set of sensors we would like to know if there is an error in either of these sensors. We can do this by comparing the mean values and variance returned from each sensor. This would also indicate a possible issue with one of the engines if the values arn't very similar to one another. We utilize the Mllib package from PySpark to take these basic column statistics, then compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.stat import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.mllib.linalg import Vectors, Vector\n",
    "\n",
    "def column_stats(dataframe, column):\n",
    "  vectorAssembler = VectorAssembler(inputCols=[column],outputCol=\"features\")\n",
    "  expr = [col(c).cast(\"Double\").alias(c) for c in vectorAssembler.getInputCols()]\n",
    "  df2 = dataframe.select(*expr)\n",
    "  df = vectorAssembler.transform(df2)\n",
    "  df2 = df2.filter(df2.n1 > 0)\n",
    "  rdd = df2.map(lambda data: Vectors.dense([c for c in data]))\n",
    "  summary = Statistics.colStats(rdd)\n",
    "  return((summary.mean()[0], summary.variance()[0]))\n",
    "\n",
    "def anomalyDetect(dataframe, flight_num):\n",
    "  dataDF = dataframe.filter(dataframe.adi_flight_record_number == flight_num)\n",
    "  Engine1_data = dataDF.filter(dataDF.engine_position == 1)\n",
    "  Engine2_data = dataDF.filter(dataDF.engine_position == 2)\n",
    "\n",
    "  Engine1_stats = column_stats(Engine1_data, 'n1')\n",
    "  Engine2_stats = column_stats(Engine2_data, 'n1')\n",
    "  \n",
    "  print(\"Flight number \" + str(flight_num))\n",
    "  print(\"----------------------------------\")\n",
    "  print(\"Difference in Engine1 and Engine2 n1 Averages: \")\n",
    "  print(Engine1_stats[0] - Engine2_stats[0])\n",
    "  print('')\n",
    "  print(\"With a variance ratio of: \")\n",
    "  print(Engine1_stats[1]/Engine2_stats[1])\n",
    "  print(\"----------------------------------\")\n",
    "  \n",
    "  if(Engine1_stats[0] - Engine2_stats[0] > 1 or Engine1_stats[1]/Engine2_stats[1] > 2 or Engine1_stats[1]/Engine2_stats[1] < 1/2): # These cutoffs need to be refined via discussion with engineers\n",
    "    print(\"Sensor Anomaly Detected\")\n",
    "  else:\n",
    "    print(\"No Sensor Anomaly Detected\")\n",
    "    \n",
    "    \n",
    "anomalyDetect(dataDF, 9861128)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Use Case - Genetic Algorithms for Optimization </h1>\n",
    "\n",
    "A Genetic Algorithm is an algorithm that solves an optimization process by mimicking the idea of evolution. That is, in each generation (iteration) of our algorithm, we generate canidate solutions to our optimization problem and combine these (various a random combination operation) to generate the 'offspring'. Now the best of these offspring solutions is again combined and randomly mutated to produce offspring and so on. Eventually, when we reach a certain level of fitness (closeness to optimal solution), we terminate and return the current offspring. Below is a sample genetic algorithm for producing a partition (set of numbers that sum upto a target) of an integer X, credit goes to Will Larson at lethain.com. We can see this is equivelent to optimizing the different between the sum of a n-element list and the target integer X. After the following code, we will evaluate how such optimization can be used to refine our time-series models when run with the iSax indexed date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Example usage\n",
    "from genetic import *\n",
    "target = 371\n",
    "p_count = 100\n",
    "i_length = 6\n",
    "i_min = 0\n",
    "i_max = 100\n",
    "p = population(p_count, i_length, i_min, i_max)\n",
    "fitness_history = [grade(p, target),]\n",
    "for i in xrange(100):\n",
    "    p = evolve(p, target)\n",
    "    fitness_history.append(grade(p, target))\n",
    "\n",
    "for datum in fitness_history:\n",
    "   print datum\n",
    "\"\"\"\n",
    "from random import randint, random\n",
    "from operator import add\n",
    "\n",
    "def individual(length, min, max):\n",
    "    'Create a member of the population.'\n",
    "    return [ randint(min,max) for x in xrange(length) ]\n",
    "\n",
    "def population(count, length, min, max):\n",
    "    \"\"\"\n",
    "    Create a number of individuals (i.e. a population).\n",
    "\n",
    "    count: the number of individuals in the population\n",
    "    length: the number of values per individual\n",
    "    min: the minimum possible value in an individual's list of values\n",
    "    max: the maximum possible value in an individual's list of values\n",
    "\n",
    "    \"\"\"\n",
    "    return [ individual(length, min, max) for x in xrange(count) ]\n",
    "\n",
    "def fitness(individual, target):\n",
    "    \"\"\"\n",
    "    Determine the fitness of an individual. Higher is better.\n",
    "\n",
    "    individual: the individual to evaluate\n",
    "    target: the target number individuals are aiming for\n",
    "    \"\"\"\n",
    "    sum = reduce(add, individual, 0)\n",
    "    return abs(target-sum)\n",
    "\n",
    "def grade(pop, target):\n",
    "    'Find average fitness for a population.'\n",
    "    summed = reduce(add, (fitness(x, target) for x in pop))\n",
    "    return summed / (len(pop) * 1.0)\n",
    "\n",
    "def evolve(pop, target, retain=0.2, random_select=0.05, mutate=0.01):\n",
    "    graded = [ (fitness(x, target), x) for x in pop]\n",
    "    graded = [ x[1] for x in sorted(graded)]\n",
    "    retain_length = int(len(graded)*retain)\n",
    "    parents = graded[:retain_length]\n",
    "    # randomly add other individuals to\n",
    "    # promote genetic diversity\n",
    "    for individual in graded[retain_length:]:\n",
    "        if random_select > random():\n",
    "            parents.append(individual)\n",
    "    # mutate some individuals\n",
    "    for individual in parents:\n",
    "        if mutate > random():\n",
    "            pos_to_mutate = randint(0, len(individual)-1)\n",
    "            # this mutation is not ideal, because it\n",
    "            # restricts the range of possible values,\n",
    "            # but the function is unaware of the min/max\n",
    "            # values used to create the individuals,\n",
    "            individual[pos_to_mutate] = randint(\n",
    "                min(individual), max(individual))\n",
    "    # crossover parents to create children\n",
    "    parents_length = len(parents)\n",
    "    desired_length = len(pop) - parents_length\n",
    "    children = []\n",
    "    while len(children) < desired_length:\n",
    "        male = randint(0, parents_length-1)\n",
    "        female = randint(0, parents_length-1)\n",
    "        if male != female:\n",
    "            male = parents[male]\n",
    "            female = parents[female]\n",
    "            half = len(male) / 2\n",
    "            child = male[:half] + female[half:]\n",
    "            children.append(child)\n",
    "    parents.extend(children)\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Additional Resources </h2>\n",
    "\n",
    "<li>\n",
    "PySpark Master Documentation\n",
    "https://people.eecs.berkeley.edu/~jegonzal/pyspark/\n",
    "<br>\n",
    "\n",
    " \n",
    "<li>\n",
    "Getting Started with Spark in Python (webpage)\n",
    "https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python\n",
    "<br>\n",
    "  \n",
    "<li>\n",
    "PySpark Wordcount example (webpage)\n",
    "http://www.clouddatalab.com/spark/pyspark/wordcount.html\n",
    "<br>\n",
    " \n",
    "<li>\n",
    "PySpark Github Repository (GH)\n",
    "https://github.com/apache/spark/tree/master/python/pyspark\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "Cloudera Spark Guide (webpage)\n",
    "https://www.cloudera.com/documentation/enterprise/5-7-x/topics/spark.html\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "Getting the Best Performance from PySpark (slides)\n",
    "http://www.slideshare.net/SparkSummit/getting-the-best-performance-with-pyspark\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "Genetic Algorithms: Cool Name and Damn Simple \n",
    "http://lethain.com/genetic-algorithms-cool-name-damn-simple/\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "Tips for Writing Better Spark Programs (slides)\n",
    "http://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "name": "PySpark_Tutorial",
  "notebookId": 2213908685042863
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
